{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ce52ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pratikdomadiya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pratikdomadiya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pratikdomadiya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/pratikdomadiya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9adcc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movie review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "# Input a movie review : This movie was really amazing and I am very curious to know what will happen in the next part\n",
    "review = input(\"Enter a movie review: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "906eaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(review):  \n",
    "    # Convert review to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Code to remove the Hashtags from the text\n",
    "    review=re.sub(r'\\B#\\S+','',review)\n",
    "    # Code to remove the links from the text\n",
    "    review=re.sub(r\"http\\S+\", \"\", review)\n",
    "    # Code to remove the Special characters from the text \n",
    "    review=' '.join(re.findall(r'\\w+', review))\n",
    "    # Code to substitute the multiple spaces with single spaces\n",
    "    review=re.sub(r'\\s+', ' ', review, flags=re.I)\n",
    "    # Code to remove all the single characters in the text\n",
    "    review=re.sub(r'\\s+[a-zA-Z]\\s+', '', review)\n",
    "    # Remove the twitter handlers\n",
    "    review=re.sub('@[^\\s]+','',review)\n",
    "\n",
    "    # Function to tokenize and remove the stopwords    \n",
    "    filtered_sentence = [] \n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    word_tokens = word_tokenize(review) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "    make_sentence = ' '.join([i+' ' for i in filtered_sentence])\n",
    "    make_sentence=re.sub(r'\\s+', ' ', make_sentence, flags=re.I)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    example = make_sentence\n",
    "    output_sentence =[]\n",
    "    word_tokens2 = word_tokenize(example)\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
    "\n",
    "    # Remove characters which have length less than 2  \n",
    "    without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
    "    # Remove numbers\n",
    "    cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
    "        \n",
    "    \n",
    "    make_sentence = ' '.join([i+' ' for i in cleaned_data_title])\n",
    "    make_sentence=re.sub(r'\\s+', ' ', cleaned_data_title, flags=re.I)\n",
    "    \n",
    "    print(make_sentence)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Tokenize the review into words\n",
    "#     words = word_tokenize(review)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     # Remove stopwords and special characters\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     punctuations = set(string.punctuation)\n",
    "#     filtered_words = [word for word in words if word not in stop_words and word not in punctuations]\n",
    "\n",
    "#     # Initialize stemmer and lemmatizer objects\n",
    "#     stemmer = PorterStemmer()\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#     # Define a function to convert nltk pos tags to wordnet pos tags\n",
    "#     def get_wordnet_pos(nltk_pos_tag):\n",
    "#         if nltk_pos_tag.startswith('J'):\n",
    "#             return nltk.corpus.wordnet.ADJ\n",
    "#         elif nltk_pos_tag.startswith('V'):\n",
    "#             return nltk.corpus.wordnet.VERB\n",
    "#         elif nltk_pos_tag.startswith('N'):\n",
    "#             return nltk.corpus.wordnet.NOUN\n",
    "#         elif nltk_pos_tag.startswith('R'):\n",
    "#             return nltk.corpus.wordnet.ADV\n",
    "#         else:\n",
    "#             return nltk.corpus.wordnet.NOUN  # Default to noun if unable to map to wordnet pos tag\n",
    "\n",
    "#     # Iterate through the filtered words in the review and stem and lemmatize each word\n",
    "#     stemmed_words = []\n",
    "#     lemmatized_words = []\n",
    "#     for word in filtered_words:\n",
    "#         # Perform stemming\n",
    "#         stemmed_word = stemmer.stem(word)\n",
    "#         stemmed_words.append(stemmed_word)\n",
    "\n",
    "#         # Perform lemmatization\n",
    "#         nltk_pos_tag = nltk.pos_tag([word])[0][1]  # Get the part of speech tag using nltk pos_tag\n",
    "#         wordnet_pos_tag = get_wordnet_pos(nltk_pos_tag)  # Map nltk pos tag to wordnet pos tag\n",
    "#         lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos_tag)\n",
    "#         lemmatized_words.append(lemmatized_word)\n",
    "\n",
    "    # Print the filtered, stemmed, and lemmatized words\n",
    "#     print(\"Filtered words: \", filtered_words)\n",
    "#     print(\"Stemmed words: \", stemmed_words)\n",
    "    print(\"Lemmatized words: \", make_sentence)\n",
    "    return make_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb8abe79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Combine the filtered, stemmed and lemmatized words into sentences\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# filtered_sentence = ' '.join(filtered_words)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# stemmed_sentence = ' '.join(stemmed_words)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m lemmatized_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f'Filtered sentence: {filtered_sentence}')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(f'Stemmed sentence: {stemmed_sentence}')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatized sentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlemmatized_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     32\u001b[0m output_sentence \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     33\u001b[0m word_tokens2 \u001b[38;5;241m=\u001b[39m word_tokenize(example)\n\u001b[0;32m---> 34\u001b[0m lemmatized_output \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens2]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Remove characters which have length less than 2  \u001b[39;00m\n\u001b[1;32m     37\u001b[0m without_single_chr \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m lemmatized_output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m output_sentence \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     33\u001b[0m word_tokens2 \u001b[38;5;241m=\u001b[39m word_tokenize(example)\n\u001b[0;32m---> 34\u001b[0m lemmatized_output \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241m.\u001b[39mlemmatize(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokens2]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Remove characters which have length less than 2  \u001b[39;00m\n\u001b[1;32m     37\u001b[0m without_single_chr \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m lemmatized_output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine the filtered, stemmed and lemmatized words into sentences\n",
    "# filtered_sentence = ' '.join(filtered_words)\n",
    "# stemmed_sentence = ' '.join(stemmed_words)\n",
    "lemmatized_sentence = ' '.join(preprocess_text(review))\n",
    "\n",
    "# print(f'Filtered sentence: {filtered_sentence}')\n",
    "# print(f'Stemmed sentence: {stemmed_sentence}')\n",
    "print(f'Lemmatized sentence: {lemmatized_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0347e4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1933465437.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [19]\u001b[0;36m\u001b[0m\n\u001b[0;31m    one reviewer mention watch 1 oz episode 'll hooked right exactly happen me. br br first thing struck oz\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "one reviewer mention watch 1 oz episode 'll hooked right exactly happen me. br br first thing struck oz \n",
    "brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard \n",
    "drug sex violence hardcore classic use word. br br call oz nickname give oswald maximum security state penitentary \n",
    "focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home\n",
    "many .. aryan muslim gangsta latino christian italian irish .... scuffle death stare dodgy dealing shady agreement \n",
    "never far away. br br would say main appeal show due fact go show would n't dare forget pretty picture paint\n",
    "mainstream audience forget charm forget romance ... oz n't mess around first episode ever saw struck nasty surreal \n",
    "could n't say ready watch developed taste oz get accustom high level graphic violence violence injustice crooked \n",
    "guard 'll sell nickel inmate 'll kill order get away well mannered middle class inmate turn prison bitch due lack \n",
    "street skill prison experience watch oz may become comfortable uncomfortable view .... thats get touch darker side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e02209",
   "metadata": {},
   "outputs": [],
   "source": [
    "one reviewer mentioned watching episode hooked right exactly happened first thing struck brutality unflinching \n",
    "scene violence set right word trust notshow faint hearted timid show pull punch regard drug sex violence hardcore \n",
    "classic use word called nickname given oswald maximum security state penitentary focus mainly emerald city \n",
    "experimental section prison cell glass front face inwards privacy high agenda city home many aryan muslim gangsta \n",
    "latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away brwould say main \n",
    "appeal show due fact show wouldndare forget pretty picture painted mainstream audience forget charm forget romance \n",
    "doesnmess around first episodeever saw struck nasty surrealcouldnsaywas ready aswatched moredevelopedtaste got \n",
    "accustomed high level graphic violence violence injustice crooked guard sold fornickel inmate kill order get away \n",
    "well mannered middle class inmate turned prison bitch due lack street skill prison experience watching may become \n",
    "comfortable uncomfortable viewing thats get touch darker side "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
